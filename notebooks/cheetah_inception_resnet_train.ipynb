{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using the Inception-ResNet-v2\n",
    "\n",
    "This notebook is based on the code from Benjamin Wild, with modifications and additions.\n",
    "\n",
    "First, we load the required dependencies and set the paths to the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/benjamin/Daten/Data/Gepardenvisionsystem_IZW/notebooks\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "import os\n",
    "import sys\n",
    "base_path = os.path.abspath(os.pardir)\n",
    "sys.path.append(base_path)\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "os.chdir(base_path + '/data')\n",
    "data_path = 'model_data_extended/'\n",
    "train_data_path = data_path + 'train/'\n",
    "val_data_path = data_path + 'val/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-24T11:33:08.951174Z",
     "start_time": "2017-07-24T13:33:08.877104+02:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session, get_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.layers import Dense, Dropout, Input, concatenate\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Nadam\n",
    "from scipy.ndimage.interpolation import rotate\n",
    "from sklearn.metrics import precision_recall_curve, classification_report, accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/home/benjamin/Software/miniconda3/envs/test_env/lib/python36.zip', '/home/benjamin/Software/miniconda3/envs/test_env/lib/python3.6', '/home/benjamin/Software/miniconda3/envs/test_env/lib/python3.6/lib-dynload', '/home/benjamin/Software/miniconda3/envs/test_env/lib/python3.6/site-packages', '/home/benjamin/Software/miniconda3/envs/test_env/lib/python3.6/site-packages/IPython/extensions', '/home/benjamin/.ipython', '/media/benjamin/Daten/Data/Gepardenvisionsystem_IZW']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)\n",
    "\n",
    "# Monkey-patch keras DirectoryIterator to also return filename\n",
    "import keras\n",
    "import keras_preprocessing\n",
    "from notebooks.keras_util_v2.util import DirectoryIteratorWithFname\n",
    "keras_preprocessing.image.DirectoryIterator = DirectoryIteratorWithFname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data loading\n",
    "\n",
    "We load the text file containing the labels used in ImageNet. Cheetahs and Leopards are already present there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and parse ImageNet class labels\n",
    "\n",
    "#classes = open(base_path + '/modules/imagenet_classes', 'r').readlines()\n",
    "classes = open('imagenet_classes', 'r').readlines()\n",
    "\n",
    "def strip(c):\n",
    "    key, value = c.split(':')\n",
    "    key = key.strip()\n",
    "    key = key.split('{')[-1]\n",
    "    value = value.split(\"'\")[1].strip()\n",
    "    return int(key), value\n",
    "\n",
    "classes = dict([strip(c) for c in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "izw_classes = ('unknown', 'cheetah', 'leopard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We load the metadata (which contains the file paths) generated in the `split_data` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43118 4729\n",
      "43120 4736 0 0\n",
      "43120 4732 0 0\n"
     ]
    }
   ],
   "source": [
    "metadata = pd.read_hdf(data_path + 'metadata.hdf5')\n",
    "val_metadata = metadata[metadata.set=='val'].copy()\n",
    "\n",
    "# configure the batch size\n",
    "batch_size = 16\n",
    "small_batch_size = 4\n",
    "\n",
    "train_size, val_size = sum(metadata.set=='train'), sum(metadata.set=='val')\n",
    "train_pad, val_pad = train_size + (batch_size - (train_size % batch_size)), val_size + (batch_size - (val_size % batch_size))\n",
    "train_pad_small, val_pad_small = train_size + (small_batch_size - (train_size % small_batch_size)), val_size + (small_batch_size - (val_size % small_batch_size))\n",
    "\n",
    "print(train_size, val_size)\n",
    "print(train_pad, val_pad, train_pad % batch_size, val_pad % batch_size)\n",
    "print(train_pad_small, val_pad_small, train_pad_small % small_batch_size, val_pad_small % small_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ambient_temp</th>\n",
       "      <th>brightness</th>\n",
       "      <th>contrast</th>\n",
       "      <th>datetime</th>\n",
       "      <th>event1</th>\n",
       "      <th>event2</th>\n",
       "      <th>filename</th>\n",
       "      <th>hour</th>\n",
       "      <th>path</th>\n",
       "      <th>saturation</th>\n",
       "      <th>...</th>\n",
       "      <th>serial_no</th>\n",
       "      <th>sharpness</th>\n",
       "      <th>event_key_simple</th>\n",
       "      <th>sortkey</th>\n",
       "      <th>label</th>\n",
       "      <th>set</th>\n",
       "      <th>duplicates</th>\n",
       "      <th>timeoffset</th>\n",
       "      <th>event_key</th>\n",
       "      <th>set_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>2015-05-12 05:16:20</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>Leo_Omatewa_000408.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>source_data_v2/leopard/Leo_Omatewa_000408.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>H600HG01173547</td>\n",
       "      <td>32</td>\n",
       "      <td>H600HG01173547_2015_132_61</td>\n",
       "      <td>H600HG01173547_2015_132_611431407780000000000</td>\n",
       "      <td>leopard</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>H600HG01173547_2015_132_61</td>\n",
       "      <td>model_data_extended/train/leopard/leopard_0.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>2015-05-12 05:16:22</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>Leo_Omatewa_000409.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>source_data_v2/leopard/Leo_Omatewa_000409.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>H600HG01173547</td>\n",
       "      <td>32</td>\n",
       "      <td>H600HG01173547_2015_132_61</td>\n",
       "      <td>H600HG01173547_2015_132_611431407782000000000</td>\n",
       "      <td>leopard</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0 days 00:00:02</td>\n",
       "      <td>H600HG01173547_2015_132_61</td>\n",
       "      <td>model_data_extended/train/leopard/leopard_1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>2017-05-08 18:56:30</td>\n",
       "      <td>0</td>\n",
       "      <td>587</td>\n",
       "      <td>Eska03_Leo_B_000020.jpg</td>\n",
       "      <td>18</td>\n",
       "      <td>source_data_v2/leopard/Eska03_Leo_B_000020.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>H600HG01173547</td>\n",
       "      <td>32</td>\n",
       "      <td>H600HG01173547_2017_128_587</td>\n",
       "      <td>H600HG01173547_2017_128_5871494269790000000000</td>\n",
       "      <td>leopard</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>727 days 13:40:08</td>\n",
       "      <td>H600HG01173547_2017_128_587</td>\n",
       "      <td>model_data_extended/train/leopard/leopard_2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>2017-05-08 18:56:30</td>\n",
       "      <td>0</td>\n",
       "      <td>587</td>\n",
       "      <td>Leopard_000051.jpg</td>\n",
       "      <td>18</td>\n",
       "      <td>source_data_v2/leopard/Leopard_000051.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>H600HG01173547</td>\n",
       "      <td>32</td>\n",
       "      <td>H600HG01173547_2017_128_587</td>\n",
       "      <td>H600HG01173547_2017_128_5871494269790000000000</td>\n",
       "      <td>leopard</td>\n",
       "      <td>none</td>\n",
       "      <td>1</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>H600HG01173547_2017_128_587</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>2017-05-08 18:56:31</td>\n",
       "      <td>0</td>\n",
       "      <td>587</td>\n",
       "      <td>Eska03_Leo_B_000021.jpg</td>\n",
       "      <td>18</td>\n",
       "      <td>source_data_v2/leopard/Eska03_Leo_B_000021.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>H600HG01173547</td>\n",
       "      <td>32</td>\n",
       "      <td>H600HG01173547_2017_128_587</td>\n",
       "      <td>H600HG01173547_2017_128_5871494269791000000000</td>\n",
       "      <td>leopard</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>0 days 00:00:01</td>\n",
       "      <td>H600HG01173547_2017_128_587</td>\n",
       "      <td>model_data_extended/train/leopard/leopard_4.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ambient_temp  brightness  contrast            datetime  event1  event2  \\\n",
       "0             4           0       160 2015-05-12 05:16:20       0      61   \n",
       "1             4           0       160 2015-05-12 05:16:22       0      61   \n",
       "2            14           0       160 2017-05-08 18:56:30       0     587   \n",
       "3            14           0       160 2017-05-08 18:56:30       0     587   \n",
       "4            14           0       160 2017-05-08 18:56:31       0     587   \n",
       "\n",
       "                  filename  hour  \\\n",
       "0   Leo_Omatewa_000408.jpg     5   \n",
       "1   Leo_Omatewa_000409.jpg     5   \n",
       "2  Eska03_Leo_B_000020.jpg    18   \n",
       "3       Leopard_000051.jpg    18   \n",
       "4  Eska03_Leo_B_000021.jpg    18   \n",
       "\n",
       "                                             path  saturation  \\\n",
       "0   source_data_v2/leopard/Leo_Omatewa_000408.jpg           0   \n",
       "1   source_data_v2/leopard/Leo_Omatewa_000409.jpg           0   \n",
       "2  source_data_v2/leopard/Eska03_Leo_B_000020.jpg           0   \n",
       "3       source_data_v2/leopard/Leopard_000051.jpg           0   \n",
       "4  source_data_v2/leopard/Eska03_Leo_B_000021.jpg           0   \n",
       "\n",
       "                        ...                              serial_no  sharpness  \\\n",
       "0                       ...                         H600HG01173547         32   \n",
       "1                       ...                         H600HG01173547         32   \n",
       "2                       ...                         H600HG01173547         32   \n",
       "3                       ...                         H600HG01173547         32   \n",
       "4                       ...                         H600HG01173547         32   \n",
       "\n",
       "              event_key_simple  \\\n",
       "0   H600HG01173547_2015_132_61   \n",
       "1   H600HG01173547_2015_132_61   \n",
       "2  H600HG01173547_2017_128_587   \n",
       "3  H600HG01173547_2017_128_587   \n",
       "4  H600HG01173547_2017_128_587   \n",
       "\n",
       "                                          sortkey    label    set duplicates  \\\n",
       "0   H600HG01173547_2015_132_611431407780000000000  leopard  train          0   \n",
       "1   H600HG01173547_2015_132_611431407782000000000  leopard  train          0   \n",
       "2  H600HG01173547_2017_128_5871494269790000000000  leopard  train          1   \n",
       "3  H600HG01173547_2017_128_5871494269790000000000  leopard   none          1   \n",
       "4  H600HG01173547_2017_128_5871494269791000000000  leopard  train          1   \n",
       "\n",
       "         timeoffset                    event_key  \\\n",
       "0               NaT   H600HG01173547_2015_132_61   \n",
       "1   0 days 00:00:02   H600HG01173547_2015_132_61   \n",
       "2 727 days 13:40:08  H600HG01173547_2017_128_587   \n",
       "3   0 days 00:00:00  H600HG01173547_2017_128_587   \n",
       "4   0 days 00:00:01  H600HG01173547_2017_128_587   \n",
       "\n",
       "                                          set_path  \n",
       "0  model_data_extended/train/leopard/leopard_0.jpg  \n",
       "1  model_data_extended/train/leopard/leopard_1.jpg  \n",
       "2  model_data_extended/train/leopard/leopard_2.jpg  \n",
       "3                                              NaN  \n",
       "4  model_data_extended/train/leopard/leopard_4.jpg  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The Keras image generators read the training and validation sets and supply a stream of image batches of the classes. (They infer the image classes from the directory names.)  \n",
    "The training image iterator additionally augments the dataset by flipping images, the validation image iterator does not. \n",
    "\n",
    "Additionally there's a preprocessing generator wrapped around the iterator which augments the image input data with time/temperature metadata and additionally rotates images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop camera metainformation from images\n",
    "\n",
    "def preprocess(data, batch_size, rotate_range=None):\n",
    "    for x, y, fns in data:\n",
    "        batch_metadata = []\n",
    "        for fname in fns:\n",
    "            fname_splitted = fname.split('_')[1].split('.')\n",
    "            \n",
    "            index = fname_splitted[0]\n",
    "            #rest = '_'.join(fname_splitted[1:]).split('.jpeg')[0]\n",
    "            f_metadata = metadata.loc[int(index)]\n",
    "            \n",
    "            batch_metadata.append((\n",
    "                f_metadata.ambient_temp,\n",
    "                f_metadata.hour))\n",
    "            # optionally use metadata\n",
    "        temperatures = np.array(batch_metadata).astype(np.float32)\n",
    "        x = x[:, 10:-10, 10:-10, :]\n",
    "        if rotate_range is not None:\n",
    "            for idx in range(x.shape[0]):\n",
    "                x[idx] = rotate(x[idx], np.random.random() * rotate_range * 2 - rotate_range, \n",
    "                                order=1, mode='reflect', reshape=False).astype(np.int64)\n",
    "        yield [preprocess_input(x), temperatures], y\n",
    "        #yield [2*((x/255)-0.5), temperatures], y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Because training the whole model with 32 image batches doesn't fit our 4GB GPU, we specifiy additional generators for batches of size 4.\n",
    "\n",
    "The top layers can be trained with 32 size batches, but the finetuning only works on 4 image batches. Adjust if trained on a better GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43118 images belonging to 3 classes.\n",
      "Found 4729 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Augment train data with horizontal flips, scale to ImageNet input size\n",
    "\n",
    "generator = ImageDataGenerator(horizontal_flip=True)\n",
    "val_generator = ImageDataGenerator(horizontal_flip=False)\n",
    "\n",
    "train_gen = preprocess(generator.flow_from_directory(\n",
    "    train_data_path, \n",
    "    target_size=(299+20, 299+20),\n",
    "    classes=izw_classes,\n",
    "    batch_size=batch_size), batch_size, rotate_range=10)\n",
    "\n",
    "val_gen = preprocess(val_generator.flow_from_directory(\n",
    "    val_data_path, \n",
    "    target_size=(299+20, 299+20),\n",
    "    classes=izw_classes,\n",
    "    batch_size=batch_size), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_generator = ImageDataGenerator(horizontal_flip=True)\n",
    "small_val_generator = ImageDataGenerator(horizontal_flip=False)\n",
    "\n",
    "train_gen_small = preprocess(small_generator.flow_from_directory(\n",
    "    train_data_path, \n",
    "    target_size=(299+20, 299+20),\n",
    "    classes=izw_classes,\n",
    "    batch_size=small_batch_size), small_batch_size, rotate_range=10)\n",
    "\n",
    "val_gen_small = preprocess(small_val_generator.flow_from_directory(\n",
    "    val_data_path, \n",
    "    target_size=(299+20, 299+20),\n",
    "    classes=izw_classes,\n",
    "    batch_size=small_batch_size), small_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Test pretrained model\n",
    "\n",
    "To get a classification baseline of the neural net without finetuning, we just try to classify with some pictures.  \n",
    "It generally works somewhat, but the NN has a harder job than necessary because the best estimates sometimes are labels we are not interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 1.001\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        s = '{:.3f}'.format(cm[i, j]) if normalize else cm[i,j]\n",
    "        plt.text(j, i, s,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def label_confusion_matrix(val_labels, val_preds):\n",
    "    cnf_matrix = confusion_matrix(val_labels, val_preds)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plot_confusion_matrix(cnf_matrix, classes=izw_classes,\n",
    "                          normalize=False, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "#\n",
    "# http://arxiv.org/abs/1602.07261\n",
    "#\n",
    "# Inception-v4, Inception-ResNet and the Impact of Residual Connections\n",
    "# on Learning\n",
    "#\n",
    "# Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n",
    "\n",
    "model = InceptionResNetV2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = []\n",
    "val_preds = []\n",
    "\n",
    "for idx, (batch, labels) in enumerate(val_gen):\n",
    "    pred = model.predict(batch[0])\n",
    "    for i in range(pred.shape[0]):    \n",
    "        class_pred = classes[pred[i,:].argmax()]\n",
    "        class_pred = np.where([x in class_pred for x in izw_classes])\n",
    "        class_pred = 0 if class_pred[0].size==0 else class_pred[0][0]\n",
    "        \n",
    "        val_preds.append(class_pred)\n",
    "        val_labels.append(labels[i].argmax())\n",
    "    \n",
    "    if idx == (val_size // batch_size):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.338%\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    unknown       0.34      1.00      0.51       629\n",
      "    cheetah       0.94      0.71      0.81      3393\n",
      "    leopard       0.76      0.36      0.49       707\n",
      "\n",
      "avg / total       0.83      0.69      0.72      4729\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Accuracy: {:.3f}%\\n'.format(accuracy_score(val_labels, val_preds) * 100))\n",
    "print(classification_report(val_labels, val_preds, target_names=izw_classes))\n",
    "\n",
    "label_confusion_matrix(val_labels, val_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model definition and finetuning\n",
    "\n",
    "First we define the model: Based on the InceptionResnetV2, we remove its last fully connected layer.  \n",
    "A new final layer is added that takes the previous hidden layer output and the metadata information. Dropout is used for regularization. \n",
    "\n",
    "We only have a 3-class softmax output for the Cheetah, Leopard and Unknown classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pretrained model, but skip last (classifier) layer and replace it with a new layer for the three IZW classes\n",
    "# Fix pretrained layers, only learn last weights for last layer\n",
    "# Also use metadata as additional input\n",
    "\n",
    "metadata_input = Input(shape=(2, ))\n",
    "\n",
    "base_model = InceptionResNetV2(include_top=False, pooling='avg')\n",
    "h = concatenate([base_model.output, metadata_input])\n",
    "h = Dropout(.2, name='Dropout')(h)\n",
    "outputs = Dense(3, activation='softmax')(h)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model = Model(base_model.inputs + [metadata_input], outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(base_path + '/data/model_data/inception-resnet-v2-cheetahs_weights.h5')\n",
    "optim = Nadam(0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.save(\"/home/benjamin/Entwicklung/ComputerVision/Gepardenvisionsystem_IZW/reconyx_classifier/model/inception-resnet-v2-cheetahs_36.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Training the last layer is straightforward, but finetuning the whole model afterwards can be difficult since it requires a GPU with much memory.\n",
    "\n",
    "Here we train the model, evaluate the results and store the weights and the model on disk.\n",
    "\n",
    "The cells are currently deactivated unless we want to retrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Nadam(0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "model.fit_generator(train_gen, steps_per_epoch=train_pad / batch_size, validation_data=(val_gen), validation_steps=val_pad / batch_size,\n",
    "                    epochs=1, workers=1, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Now finetune the whole model for five more epochs with reduced learning rate\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "optim = Nadam(0.0001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "model.fit_generator(train_gen_small, steps_per_epoch=train_pad_small / small_batch_size, validation_data=(val_gen_small), validation_steps=val_pad_small / small_batch_size,\n",
    "                    epochs=5, workers=1, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.evaluate_generator(val_gen_small, steps=val_pad_small / small_batch_size, workers=1, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.save(base_path + '/data/model_data/inception-resnet-v2-cheetahs.h5')\n",
    "#model.save_weights(base_path + '/data/model_data/inception-resnet-v2-cheetahs_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluation\n",
    "\n",
    "The model now seems to more reliably detect the labels we are interested in (it's easier to decide among three classes than 1000).\n",
    "We store the class probabilities of the predictions in the validation dataframe copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_metadata['predict_probs'] = 0\n",
    "val_metadata['predict_probs'] = val_metadata['predict_probs'].astype(object)\n",
    "\n",
    "for event_key, group in val_metadata.groupby('event_key_simple'):\n",
    "    batch_x = np.zeros((len(group), 299+20, 299+20, 3), dtype=K.floatx())\n",
    "    batch_t = np.zeros((len(group), ), dtype=K.floatx())\n",
    "    batch_h = np.zeros((len(group), ), dtype=K.floatx())\n",
    "    group_iterator = enumerate(zip(group.index, group.set_path, group.ambient_temp, group.hour))\n",
    "    for i, (file_idx, path, temp, hour) in group_iterator:\n",
    "        file_path = os.path.join(*([val_data_path] + path.split('/')[-2:]))\n",
    "        img = load_img(file_path, grayscale=False, target_size=(299+20, 299+20))\n",
    "        x = img_to_array(img)\n",
    "        x = val_generator.random_transform(x.astype(K.floatx()))\n",
    "        x = val_generator.standardize(x)\n",
    "        batch_x[i] = x\n",
    "        batch_t[i] = temp\n",
    "        batch_h[i] = hour\n",
    "    batch_x = batch_x[:, 10:-10, 10:-10, :]\n",
    "    batch_x = [preprocess_input(batch_x), np.stack((batch_t, batch_h), axis=1)]\n",
    "    pred = model.predict_on_batch(batch_x)\n",
    "    \n",
    "    for i, (row_index, row) in enumerate(group.iterrows()):\n",
    "        val_metadata.at[row_index, 'predict_probs'] = pred[i,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a preliminary analysis on the single images (ignoring the correlated 3-image events) and estimate our new accuracy, which is also much improved compared to the naive model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "izw_class_arr = np.array(izw_classes)\n",
    "\n",
    "val_preds = val_metadata['predict_probs'].apply(np.argmax).as_matrix()\n",
    "val_probs = np.vstack(val_metadata['predict_probs'])\n",
    "val_labels = val_metadata['label'].apply(lambda x: np.where(x == izw_class_arr)[0][0]).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: {:.3f}%\\n'.format(accuracy_score(val_labels, val_preds) * 100))\n",
    "print(classification_report(val_labels, val_preds, target_names=izw_classes))\n",
    "\n",
    "label_confusion_matrix(val_labels, val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheetah_val_probs = val_probs[:, izw_classes.index('cheetah')]\n",
    "cheetah_val_labels = val_labels == izw_classes.index('cheetah')\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(cheetah_val_labels, cheetah_val_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-notebook')\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision)\n",
    "plt.plot(recall[precision > .999], precision[precision > .999], c='green')\n",
    "plt.plot(recall[recall > .999], precision[recall > .999], c='green')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.title('Precision-Recall curve for cheetahs')\n",
    "plt.ylim([precision.min()-0.01, precision.max()+0.01])\n",
    "plt.xlim([-0.05, 1.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_max_recall = precision[np.where(recall > .999)[0][-1]]\n",
    "print('Precision for cheetahs for recall > 99.9%: {:.1f}%'.format(precision_max_recall * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Classification analysis - simple events\n",
    "\n",
    "Individual images are classified quite well. We now want to explore how series of events can be optimally classified.\n",
    "\n",
    "We explore three strategies:\n",
    "\n",
    "- Use the image with the highest probability classification as the label for the 3-image events.\n",
    "- Sum the class probabilities in the 3-image events and take the highest probabilty. (Amounts to averaging over the images.)\n",
    "- Label the 3 images separately and majority vote the label, break ties among 3 classes by probability.\n",
    "\n",
    "First, we explore the prevalence of 3-image events with conflicting predicted labels according the the maximum class probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_labels(data_group):\n",
    "    # count the number of events where we have different labels\n",
    "    label_counts = np.array([0,0,0,0])\n",
    "    # count the number of times any of the labels were correct\n",
    "    correct_label = np.array([0,0,0,0])\n",
    "    \n",
    "    event_lens = np.array([0,0,0,0])\n",
    "\n",
    "    for index, group in data_group:\n",
    "        group_label = izw_classes.index(group['label'].iloc[0])\n",
    "        labels = group['predict_probs'].apply(np.argmax).unique()\n",
    "        label_count = len(labels)\n",
    "        label_counts[label_count] += 1\n",
    "        event_lens[label_count] += len(group)\n",
    "        correct_label[label_count] += 1 if group_label in labels else 0\n",
    "        \n",
    "    return label_counts, correct_label, event_lens\n",
    "\n",
    "label_counts, correct_label, event_lens = get_event_labels(val_metadata.groupby('event_key_simple'))\n",
    "print(\"Observed label counts: {}\".format(label_counts))\n",
    "print(\"Correct label in set:  {}\".format(correct_label))\n",
    "print(\"Correct label in set:  {}\".format(event_lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Unique-label event accuracy\n",
    "Most of the events only have one label, of which almost all are correct. Of the few events which have 2 or even 3 labels, the correct label is at least among the options (which is of course trivially true for the 3 label cases).\n",
    "\n",
    "We can't do much about the events where all images are mislabeled, but we can look at the confusion matrix for these. It shows that 2 images are false positives (a harmless case) while the others, except 1 missed Leopard, confuse Cheetahs and Leopards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_label_predictions(data_group, normalize_counts = False):\n",
    "    group_labels, group_preds = [], []\n",
    "    for index, group in data_group:\n",
    "        labels = group['predict_probs'].apply(np.argmax).unique()\n",
    "        group_label = izw_classes.index(group['label'].iloc[0])\n",
    "        if len(labels) == 1:     \n",
    "            factor = len(group) if normalize_counts else 1\n",
    "            group_preds.extend([labels[0]] * factor)\n",
    "            group_labels.extend([group_label] * factor)\n",
    "            \n",
    "    return group_labels, group_preds\n",
    "\n",
    "group_labels, group_preds = unique_label_predictions(val_metadata.groupby('event_key_simple'))\n",
    "print('Accuracy: {:.3f}%\\n'.format(accuracy_score(group_labels, group_preds) * 100))\n",
    "print(classification_report(group_labels, group_preds, target_names=izw_classes, digits=4))\n",
    "\n",
    "label_confusion_matrix(group_labels, group_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_labels, group_preds = unique_label_predictions(val_metadata.groupby('event_key_simple'), normalize_counts=True)\n",
    "print('Accuracy: {:.3f}%\\n'.format(accuracy_score(group_labels, group_preds) * 100))\n",
    "print(classification_report(group_labels, group_preds, target_names=izw_classes, digits=4))\n",
    "\n",
    "label_confusion_matrix(group_labels, group_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Conflicting-label event accuracy\n",
    "We now try the labeling strategies (maximum probability vs. sum probabilities) for the muliply labeled cases.\n",
    "\n",
    "It seems as if the selection based on the maximum class probability for any of the 3 images gives better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_label_predictions(data_group, normalize_counts = False):\n",
    "    group_labels_all, group_preds_all, group_labels, group_preds1, group_preds2 = [], [], [], [], []\n",
    "    for index, group in data_group:\n",
    "        group_probs = np.vstack(group['predict_probs'])\n",
    "        labels = group['predict_probs'].apply(np.argmax).unique()\n",
    "        group_label = izw_classes.index(group['label'].iloc[0])\n",
    "        factor = len(group) if normalize_counts else 1\n",
    "\n",
    "        if len(labels) > 1:\n",
    "            group_labels_all.extend([group_label]*factor)\n",
    "            group_labels.extend([group_label]*factor)\n",
    "            label1 = np.unravel_index(group_probs.argmax(), group_probs.shape)[1]\n",
    "            label2 = np.sum(group_probs, axis=0).argmax()\n",
    "            group_preds_all.extend([label1]*factor)\n",
    "            group_preds1.extend([label1]*factor)\n",
    "            group_preds2.extend([label2]*factor)\n",
    "        else:\n",
    "            group_labels_all.extend([group_label]*factor)\n",
    "            group_preds_all.extend([labels[0]]*factor)\n",
    "            \n",
    "    return group_labels_all, group_preds_all, group_labels, group_preds1, group_preds2\n",
    "        \n",
    "group_labels_all1, group_preds_all1, group_labels, group_preds1, group_preds2 = all_label_predictions(val_metadata.groupby('event_key_simple'), normalize_counts=False)\n",
    "print('\\t=========   Normalized   =========')\n",
    "print('Accuracy: {:.3f}%\\n'.format(accuracy_score(group_labels, group_preds1) * 100))\n",
    "print(classification_report(group_labels, group_preds1, target_names=izw_classes, digits=6))\n",
    "label_confusion_matrix(group_labels, group_preds1)\n",
    "\n",
    "group_labels_all2, group_preds_all2, group_labels, group_preds1, group_preds2 = all_label_predictions(val_metadata.groupby('event_key_simple'), normalize_counts=True)\n",
    "print('Accuracy: {:.3f}%\\n'.format(accuracy_score(group_labels, group_preds2) * 100))\n",
    "print(classification_report(group_labels, group_preds2, target_names=izw_classes, digits=6))\n",
    "label_confusion_matrix(group_labels, group_preds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Overall event accuracy\n",
    "The accuracy for the whole dataset is then around 99%, on an event-level resolution.\n",
    "\n",
    "Even for the individual images in the events, the accuracy is increased due to the conflicting label resolution via maximum event class probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: {:.3f}%\\n'.format(accuracy_score(group_labels_all1, group_preds_all1) * 100))\n",
    "print(classification_report(group_labels_all1, group_preds_all1, target_names=izw_classes))\n",
    "label_confusion_matrix(group_labels_all1, group_preds_all1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: {:.3f}%\\n'.format(accuracy_score(group_labels_all2, group_preds_all2) * 100))\n",
    "print(classification_report(group_labels_all2, group_preds_all2, target_names=izw_classes))\n",
    "label_confusion_matrix(group_labels_all2, group_preds_all2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Classification analysis - extended events\n",
    "\n",
    "Exploiting correlations in the 3-image events allowed us to improve the accuracy compared to single-image classification. In the preprocessing phase, we connected 3-image events that occurred in succession (since they likely capture the same extended event).  \n",
    "We can now try if the longer events let us reduce the error rate further - if an event has a Leopard classification but the events before and after detected a Cheetah, we might be able to correct the Leopard misclassification.\n",
    "\n",
    "However, we have to be careful with very long events, since it might happen that the presence two different animals really overlapped. (Should check ecological plausibility here.)\n",
    "\n",
    "For now, we just use the same strategies as for the 3-image events: Classify extended events by the most confident prediction among all images or the sum of all probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_label_counts, ext_correct_label, ext_event_lens = get_event_labels(val_metadata.groupby('event_key'))\n",
    "print(\"Observed label counts: {}\".format(ext_label_counts))\n",
    "print(\"Correct label in set:  {}\".format(ext_correct_label))\n",
    "print(\"Num. of events in set: {}\".format(ext_event_lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Unique-label event accuracy\n",
    "\n",
    "The unique-label accuracy is increased for extended events compared to 3-image events. This is counter-intuitive, since non-conflicting 3-image events could be still unique in larger events.  \n",
    "But as the table above shows, some unique 3-events were merged into other events that were already conflicting. Among others, 3 Cheetahs that were misclassified as Leopards were moved.\n",
    "\n",
    "The accuracy increase is thus mostly an artifact of the data here, the interest should mostly lie in how labeling conflicts in extended events are resolved (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_labels, group_preds = unique_label_predictions(val_metadata.groupby('event_key'), normalize_counts=True)\n",
    "print('Accuracy: {:.3f}%\\n'.format(accuracy_score(group_labels, group_preds) * 100))\n",
    "print(classification_report(group_labels, group_preds, target_names=izw_classes, digits=4))\n",
    "\n",
    "label_confusion_matrix(group_labels, group_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Conflicting-label event accuracy\n",
    "\n",
    "Again, classifying an event based on its most confident classification for any image gives the best results - even improved from the 3-image event resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_group_labels_all, ext_group_preds_all, ext_group_labels, ext_group_preds1, ext_group_preds2 = all_label_predictions(val_metadata.groupby('event_key'), normalize_counts=True)\n",
    "    \n",
    "print('Accuracy: {:.3f}%\\n'.format(accuracy_score(ext_group_labels, ext_group_preds1) * 100))\n",
    "print(classification_report(ext_group_labels, ext_group_preds1, target_names=izw_classes))\n",
    "\n",
    "print('Accuracy: {:.3f}%\\n'.format(accuracy_score(ext_group_labels, ext_group_preds2) * 100))\n",
    "print(classification_report(ext_group_labels, ext_group_preds2, target_names=izw_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Overall event accuracy\n",
    "Overall, using extended events slightly improves upon the 3-event image classification - and both are better than the individual image classification.\n",
    "\n",
    "HOWEVER: Note that the improvement in accuracy comes mostly from resolving the classification of 3 events (9 images), which confused Leopards and Cheetahs.  \n",
    "We should not draw overly strong conclusions from such a small sample, but the maximum event class probability mode will be used for event-mislabeling resolution. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: {:.3f}%\\n'.format(accuracy_score(ext_group_labels_all, ext_group_preds_all) * 100))\n",
    "print(classification_report(ext_group_labels_all, ext_group_preds_all, target_names=izw_classes))\n",
    "label_confusion_matrix(ext_group_labels_all, ext_group_preds_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
